# WARMONGER PROJECT.

*Доброго времени суток, всем кто это читает. Вашему вниманию представляется распределенная автоматизированная система сбора и обработки информации под названием "**Warmonger" (пер. англ. Милитарист)***

# Требования к эксплуатации

1) PostgreSQL => 10.0
2) Python => 3.8.5
3) Библиотеки из backend/requirements.txt (для работы парсера - также библиотеки из parser/requirements.txt)
4) Широкополосное подключение к ИТКС "Интернет"

# Установка

#### Подключение к БД

создайте файл CREDENTIALS.env в корне проекта, напишите в нем данные для подключения к БД PostgreSQL

`PORT = port `

`USER = user `

`PASSWORD = password `

`HOST = localhost \ ip `

`NAME = name`

#### Подключение TelethoneAPI

создайте файл TELETHONE.env в директории parser, напишите поля по образцу:

`TELETHONE_ID = your_id`
`TELETHONE_HASH = your_hash`

Узнать значения данных полей можно из документации Telethone - https://docs.telethon.dev/en/stable/

# Запуск приложения через Docker (в процессе)

Откройте терминал в корне проекта, пропишите команду `docker-compose up -d`

Через некоторое время можно зайти в браузер по адресу *http://localhost:8080/*

# Запуск приложения вручную

#### Backend

Откройте терминал в папке ***backend***, выполните команды:

`python -m venv venv`

`venv\Scripts\activate`

`pip install -r requirements.txt`

Вы создали виртуальное окружение и загрузили необходимые библиотеки, теперь можно запустить backend-часть этого проекта

В папке backend, выполнить

Для создания моделей django в БД:

`python manage.py makemigrations`

`python manage.py migrate`

Для запуска приложения:

`python manage.py runserver`

#### Telegram parser

Откройте терминал в папке parser

`python -m venv venv`

`venv\Scripts\activate`

`pip install -r requirements.txt`

Для корретной работы парсера необходимо сначала запустить django сервер, с выполненными миграциями в БД.

#### Frontend

Откройте терминал в папке ***frontend***, выполните команды:

`npm install`

`npm run serve`

После этого frontend-часть этого проекта будет доступна либо на *http://localhost:8080/*, либо на *http://localhost:8081/*

### Принцип работы проекта

В файле main.py, в директории parser вы можете конфигурировать собираемые источники - на вход принимается массив с ссылками на целевые телеграм-каналы или группы.
Если это необходимо, вы можете осуществлять поиск по ключевым словам - для этого нужно задать SearchQuery, методом setSearchQuery.

Ключевых слов может быть несколько, поэтому они подаются в виде массива строк.

Далее, парсер вычленяет именнованные сущности из собранного текста, определяет координаты локаций (при их определении), и отправляет по адресу *http://127.0.0.1:8000/main/addNews/*.

Django принимает данные и добавляет в postgreSQL.

Если Вы обладаете сетевой инфраструктурой - наилучшим решением будет поднятие инстанса парсера на машина-сборщиках и указание адреса для добавления данных в БД.

Добавление данных через django сделано в целях экономии времени, на полноценном проекте можно будет добавлять со стороны клиента напрямую в БД, с помощью библиотеки psycopg2.

### Выводы по работе и дальнейшие перспективы

На данный момент имеем standalone систему сбора и обработки информации, и приложения для администрирования и визуализации полученной информации.

В качестве визуализации предлагается анализ графов сущностей, статистические данные (топ 5 по каждой сущности) и разметка на Яндекс.картах

Перспективы разработки:

1. Повышение точности работы моделей nlp, geoint
2. Добавление автоматического определения языка новости и использования соответсвующей языковой модели
3. Расширение собираемых источников
4. Добавление новых средств сбора информации (библиотека scrapy, Selenium)
   Пометка разработчика: мы не успели доделать сбор по компаниям, но в директории Notebooks, в папке Георгия в файле pars_ukr лежит часть рабочего кода по сбору данных  о компаниях.
5. Углубление и профилирование в уже собранной информации (для пользователей и организаций)
6. Создание средства оркестрации парсеров в целях автоматизации развертывания системы

#### Screencast

Мы собрали данные с канала ukrInsider и визуализировали их с помощью нашей системы. Если вам интересно посмотреть как это выглядит - предлагаю ознакомиться с записью ниже,

*https://disk.yandex.ru/i/oKKpxNclmC715Q*

## API

Весь API представлен в файлах **views.py** и **urls.py** Django - приложения.

API предполагает передачу собранной инфомации от парсера в POST-запросе по адресу:

1. *http://127.0.0.1/main/addNews/*

Для визуализации используется следующий API к postgreSQL - GET-запросы по адресам:

1. *http://127.0.0.1:8000/main/getAllNews/*
2. *http://127.0.0.1:8000/main/getAllTags/*
3. *http://127.0.0.1:8000/main/topNews/*
4. *http://127.0.0.1:8000/main/getDetailNews/`<int:pk>`/*
5. *http://127.0.0.1:8000/main/getNewsRelations/*

API позволяет использовать приложение в любых проектах, с любой frontend-частью проекта.
